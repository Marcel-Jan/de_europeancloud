{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc346020-331b-4c41-80a6-b9718fa0e554",
   "metadata": {},
   "source": [
    "# European Parlement Data in Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cd4ff8-19f5-4002-ada4-456b19919660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyiceberg\n",
      "  Using cached pyiceberg-0.10.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting pynessie\n",
      "  Using cached pynessie-0.67.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting cachetools<7.0,>=5.5 (from pyiceberg)\n",
      "  Using cached cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (8.1.7)\n",
      "Requirement already satisfied: fsspec>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2023.9.2)\n",
      "Collecting mmh3<6.0.0,>=4.0.0 (from pyiceberg)\n",
      "  Using cached mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0 (from pyiceberg)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: pyparsing<4.0.0,>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (3.1.1)\n",
      "Collecting pyroaring<2.0.0,>=1.0.0 (from pyiceberg)\n",
      "  Using cached pyroaring-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2.31.0)\n",
      "Collecting rich<15.0.0,>=10.11.0 (from pyiceberg)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in /opt/conda/lib/python3.11/site-packages (from pyiceberg) (2.4.0)\n",
      "Collecting strictyaml<2.0.0,>=1.7.0 (from pyiceberg)\n",
      "  Using cached strictyaml-1.7.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tenacity<10.0.0,>=8.2.3 (from pyiceberg)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from pynessie) (23.1.0)\n",
      "Collecting botocore (from pynessie)\n",
      "  Downloading botocore-1.42.27-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting confuse==2.0.1 (from pynessie)\n",
      "  Using cached confuse-2.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting desert (from pynessie)\n",
      "  Using cached desert-2022.9.22-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting marshmallow (from pynessie)\n",
      "  Using cached marshmallow-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting marshmallow-oneofschema (from pynessie)\n",
      "  Using cached marshmallow_oneofschema-3.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from pynessie) (2.8.2)\n",
      "Collecting requests-aws4auth (from pynessie)\n",
      "  Using cached requests_aws4auth-1.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting simplejson (from pynessie)\n",
      "  Using cached simplejson-3.20.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from confuse==2.0.1->pynessie) (6.0.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg)\n",
      "  Using cached pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions>=4.14.1 (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=2.4.0,!=2.4.1,<3.0,>=2.0->pyiceberg)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.20.0->pyiceberg) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<15.0.0,>=10.11.0->pyiceberg)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<15.0.0,>=10.11.0->pyiceberg) (2.16.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->pynessie) (1.16.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore->pynessie)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting typing-inspect (from desert->pynessie)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<15.0.0,>=10.11.0->pyiceberg)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->desert->pynessie)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached pyiceberg-0.10.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "Using cached pynessie-0.67.0-py2.py3-none-any.whl (59 kB)\n",
      "Using cached confuse-2.0.1-py3-none-any.whl (24 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
      "Using cached mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached pyroaring-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached strictyaml-1.7.3-py3-none-any.whl (123 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading botocore-1.42.27-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached desert-2022.9.22-py3-none-any.whl (10 kB)\n",
      "Using cached marshmallow-4.2.0-py3-none-any.whl (48 kB)\n",
      "Using cached marshmallow_oneofschema-3.2.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached requests_aws4auth-1.3.1-py3-none-any.whl (24 kB)\n",
      "Using cached simplejson-3.20.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: pyroaring, typing-extensions, tenacity, simplejson, python-dotenv, mypy-extensions, mmh3, mdurl, marshmallow, jmespath, confuse, cachetools, annotated-types, typing-inspection, typing-inspect, strictyaml, requests-aws4auth, pydantic-core, marshmallow-oneofschema, markdown-it-py, botocore, rich, pydantic, desert, pynessie, pyiceberg\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed annotated-types-0.7.0 botocore-1.42.27 cachetools-6.2.4 confuse-2.0.1 desert-2022.9.22 jmespath-1.0.1 markdown-it-py-4.0.0 marshmallow-4.2.0 marshmallow-oneofschema-3.2.0 mdurl-0.1.2 mmh3-5.2.0 mypy-extensions-1.1.0 pydantic-2.12.5 pydantic-core-2.41.5 pyiceberg-0.10.0 pynessie-0.67.0 pyroaring-1.0.3 python-dotenv-1.2.1 requests-aws4auth-1.3.1 rich-14.2.0 simplejson-3.20.2 strictyaml-1.7.3 tenacity-9.1.2 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyiceberg pynessie python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd7ff97-d5d4-4354-b979-bcf3e4bcac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.3\n",
      "  Using cached pyspark-3.5.3-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.3)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19047470-fd7f-4a40-8aa7-42bc85b04f04",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de6a5be-73e5-496c-a9c9-b1f0a45ba2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencies imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType\n",
    "\n",
    "print(\"✓ Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cda26e-512d-41e4-a4bb-57457740c87d",
   "metadata": {},
   "source": [
    "## Create Spark Session with Iceberg + Nessie + S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d8ce7b-19fb-41b1-b4f5-0128817b6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.5.0 with Iceberg + Nessie + S3 configured\n",
      "  Nessie URI: http://nessie:19120/api/v1\n",
      "  S3 Endpoint: https://s3.nl-ams.scw.cloud\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create Spark session with full lakehouse configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TweedeKamer-Lakehouse\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,\"\n",
    "            \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.76.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "            \"software.amazon.awssdk:bundle:2.25.11,\"\n",
    "            \"software.amazon.awssdk:url-connection-client:2.25.11\") \\\n",
    "    .config(\"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", os.getenv(\"NESSIE_URI\")) \\\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3://lakehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", os.getenv(\"AWS_S3_ENDPOINT\")) \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", os.getenv(\"AWS_REGION\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"AWS_S3_ENDPOINT\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark {spark.version} with Iceberg + Nessie + S3 configured\")\n",
    "print(f\"  Nessie URI: {os.getenv('NESSIE_URI')}\")\n",
    "print(f\"  S3 Endpoint: {os.getenv('AWS_S3_ENDPOINT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa87f82-4b3b-471e-a3c6-3c03f3aa945f",
   "metadata": {},
   "source": [
    "## Verify Nessie Connection and Create Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8da4ec0-3b6f-4faa-9ee5-883b4a0fe54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Nessie catalog connection...\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|  analytics|\n",
      "|tweedekamer|\n",
      "|   europarl|\n",
      "|test_schema|\n",
      "+-----------+\n",
      "\n",
      "✓ Nessie catalog is accessible\n"
     ]
    }
   ],
   "source": [
    "# First, verify the Nessie catalog is available\n",
    "print(\"Checking Nessie catalog connection...\")\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN nessie\").show()\n",
    "    print(\"✓ Nessie catalog is accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error accessing Nessie catalog: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Verify NESSIE_URI is set correctly\")\n",
    "    print(\"2. Check if Nessie service is running:\")\n",
    "    print(\"   kubectl get pods -n lakehouse | grep nessie\")\n",
    "    print(\"3. Verify port-forward if running locally\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea15f7bd-c6f3-43d1-bdf1-e573cff0ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created namespace: nessie.europarl\n",
      "\n",
      "Existing namespaces in Nessie:\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|  analytics|\n",
      "|tweedekamer|\n",
      "|   europarl|\n",
      "|test_schema|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create namespace if it doesn't exist\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.europarl\")\n",
    "print(\"✓ Created namespace: nessie.europarl\")\n",
    "\n",
    "# List existing namespaces\n",
    "print(\"\\nExisting namespaces in Nessie:\")\n",
    "spark.sql(\"SHOW NAMESPACES IN nessie\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01389acb-6ab9-4fd0-b525-a7b2512f9faf",
   "metadata": {},
   "source": [
    "## Get MEP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f12907-ef70-4619-9253-e7cd332d9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MEP data...\n",
      "✓ Fetched 0 meps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Fetch all political groups (fracties) active in 2025\"\"\"\n",
    "base_url = \"https://data.europarl.europa.eu/api/v2\"\n",
    "meps_endpoint = f\"{base_url}/meps\"\n",
    "\n",
    "params = {\n",
    "    \"format\": \"application/ld+json\",\n",
    "    \"offset\": 0,\n",
    "    \"limit\": 50\n",
    "}\n",
    "\n",
    "print(\"Fetching MEP data...\")\n",
    "response = requests.get(meps_endpoint, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "data = response.json()\n",
    "meps = data.get('value', [])\n",
    "\n",
    "print(f\"✓ Fetched {len(meps)} meps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d85a80-4454-4b56-84f2-b3d313001b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset: 0\n",
      "offset: 100\n",
      "offset: 200\n",
      "offset: 300\n",
      "offset: 400\n",
      "offset: 500\n",
      "offset: 600\n",
      "offset: 700\n",
      "offset: 800\n",
      "offset: 900\n",
      "offset: 1000\n",
      "offset: 1100\n",
      "offset: 1200\n",
      "offset: 1300\n",
      "offset: 1400\n",
      "offset: 1500\n",
      "offset: 1600\n",
      "offset: 1700\n",
      "offset: 1800\n",
      "offset: 1900\n",
      "offset: 2000\n",
      "offset: 2100\n",
      "offset: 2200\n",
      "offset: 2300\n",
      "offset: 2400\n",
      "offset: 2500\n",
      "offset: 2600\n",
      "offset: 2700\n",
      "offset: 2800\n",
      "offset: 2900\n",
      "offset: 3000\n",
      "offset: 3100\n",
      "offset: 3200\n",
      "offset: 3300\n",
      "offset: 3400\n",
      "offset: 3500\n",
      "offset: 3600\n",
      "offset: 3700\n",
      "offset: 3800\n",
      "offset: 3900\n",
      "offset: 4000\n",
      "offset: 4100\n",
      "offset: 4200\n",
      "offset: 4300\n",
      "offset: 4400\n",
      "offset: 4500\n",
      "offset: 4600\n",
      "offset: 4700\n",
      "offset: 4800\n",
      "offset: 4900\n",
      "offset: 5000\n",
      "offset: 5100\n",
      "offset: 5200\n",
      "offset: 5300\n",
      "⛔ Empty body at offset 5300 — stopping.\n",
      "Fetched 5258 MEPs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "BASE = \"https://data.europarl.europa.eu/api/v2/meps\"\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/ld+json\"\n",
    "}\n",
    "\n",
    "offset = 0\n",
    "limit = 100\n",
    "all_meps = []\n",
    "\n",
    "while True:\n",
    "    print(f\"offset: {offset}\")\n",
    "    params = {\"offset\": offset, \"limit\": limit}\n",
    "    r = requests.get(BASE, headers=headers, params=params, timeout=30)\n",
    "\n",
    "    # hard stop bij lege body\n",
    "    if not r.text.strip():\n",
    "        print(f\"⛔ Empty body at offset {offset} — stopping.\")\n",
    "        break\n",
    "\n",
    "    r.raise_for_status()\n",
    "\n",
    "    try:\n",
    "        payload = r.json()\n",
    "    except Exception:\n",
    "        print(f\"⛔ Non-JSON response at offset {offset} — stopping.\")\n",
    "        break\n",
    "\n",
    "    page = payload.get(\"data\", [])\n",
    "    if not page:\n",
    "        print(f\"✓ Finished at offset {offset}\")\n",
    "        break\n",
    "    # print(f\"page: {page}\")\n",
    "\n",
    "    all_meps.extend(page)\n",
    "    offset += limit\n",
    "    time.sleep(0.2)   # throttle: voorkomt random 503s\n",
    "\n",
    "print(f\"Fetched {len(all_meps)} MEPs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad54bae-0344-4234-b5a6-96a2375e13c6",
   "metadata": {},
   "source": [
    "## Create MEPs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a32a7a0-0c35-440a-92d0-5ea497aacb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created table: nessie.europarl.meps\n"
     ]
    }
   ],
   "source": [
    "# Create table meps if it doesn't exist\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS nessie.europarl.meps (\n",
    "   id varchar(255),\n",
    "   type varchar(255),\n",
    "   identifier varchar(255),\n",
    "   label varchar(255),\n",
    "   familyName varchar(255),\n",
    "   givenName varchar(255),\n",
    "   sortLabel varchar(255)\n",
    ")\"\"\")\n",
    "print(\"✓ Created table: nessie.europarl.meps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d1735-5aa1-4da3-ad11-4d360ce44596",
   "metadata": {},
   "source": [
    "## Store data in PySpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b636435e-8d20-4a79-80c5-b1d56dee042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('type', StringType(), True),\n",
    "    StructField('identifier', StringType(), True),\n",
    "    StructField('label', StringType(), True),\n",
    "    StructField('familyName', StringType(), True),\n",
    "    StructField('givenName', StringType(), True),\n",
    "    StructField('sortLabel', StringType(), True),\n",
    "    StructField('load_ts', TimestampType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aed4d6e-55ed-4e47-8838-05805c7c8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "mep_records = [\n",
    "    (\n",
    "        mep.get('id'),\n",
    "        mep.get('type'),\n",
    "        mep.get('identifier'),\n",
    "        mep.get('label'),\n",
    "        mep.get('familyName'),\n",
    "        mep.get('givenName'),\n",
    "        mep.get('sortLabel'),\n",
    "        datetime.utcnow()      # nu past het schema exact\n",
    "    )\n",
    "    for mep in all_meps\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4160638c-cd66-4e80-9e77-dafecd2ed87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-----------------------+------------+-------------+------------+--------------------------+\n",
      "|id        |type  |identifier|label                  |familyName  |givenName    |sortLabel   |load_ts                   |\n",
      "+----------+------+----------+-----------------------+------------+-------------+------------+--------------------------+\n",
      "|person/1  |Person|1         |Georg JARZEMBOWSKI     |Jarzembowski|Georg        |JARZEMBOWSKI|2026-01-12 21:44:33.619603|\n",
      "|person/10 |Person|10        |Hendrikus VREDELING    |Vredeling   |Hendrikus    |VREDELING   |2026-01-12 21:44:33.619609|\n",
      "|person/100|Person|100       |J.C. RUTGERS           |Rutgers     |J.C.         |RUTGERS     |2026-01-12 21:44:33.619611|\n",
      "|person/101|Person|101       |Sijbrandus A. POSTHUMUS|Posthumus   |Sijbrandus A.|POSTHUMUS   |2026-01-12 21:44:33.619613|\n",
      "|person/11 |Person|11        |Hendrik J.G. WALTMANS  |Waltmans    |Hendrik J.G. |WALTMANS    |2026-01-12 21:44:33.619615|\n",
      "+----------+------+----------+-----------------------+------------+-------------+------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meps = spark.createDataFrame(mep_records, schema)\n",
    "df_meps.count(), df_meps.show(5, False)\n",
    "\n",
    "df_meps.writeTo(\"nessie.europarl.meps\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cb2b993-6080-43e2-8bc5-a083232c99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SELECT: nessie.europarl.meps\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM nessie.europarl.meps\"\"\")\n",
    "print(\"✓ SELECT: nessie.europarl.meps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cafac40-645c-488a-80b7-9d94747a5fbc",
   "metadata": {},
   "source": [
    "# Current MEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6b8b6cb-ffbb-4272-834b-f71efa766c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP TABLE nessie.europarl.current_meps\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b211f94-e6f0-4e33-8bd1-e6ee940a4bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created table: nessie.europarl.current_meps\n"
     ]
    }
   ],
   "source": [
    "# Create current_meps table if it doesn't exist\n",
    "spark.sql(\"\"\"CREATE TABLE nessie.europarl.current_meps (\n",
    "   id STRING,\n",
    "   type STRING,\n",
    "   identifier STRING,\n",
    "   label STRING,\n",
    "   familyName STRING,\n",
    "   givenName STRING,\n",
    "   sortLabel STRING,\n",
    "   country STRING,\n",
    "   political_group STRING,\n",
    "   load_ts TIMESTAMP\n",
    ")\"\"\")\n",
    "print(\"✓ Created table: nessie.europarl.current_meps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ec39c-1591-4571-a06f-da0c5b58739f",
   "metadata": {},
   "source": [
    "## Get current MEP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40583d-72a9-4455-83dc-11c7b567f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fetch all MEPs currently active (as of today)\"\"\"\n",
    "import time\n",
    "import requests\n",
    "\n",
    "BASE = \"https://data.europarl.europa.eu/api/v2/meps/show-current\"\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/ld+json\"\n",
    "}\n",
    "offset = 0\n",
    "limit = 100\n",
    "all_current = []\n",
    "\n",
    "while True:\n",
    "    params = {\"offset\": offset, \"limit\": limit}\n",
    "    print(f\"offset: {offset}\")\n",
    "    r = requests.get(BASE, headers=headers, params=params)\n",
    "    # hard stop bij lege body\n",
    "    if not r.text.strip():\n",
    "        print(f\"⛔ Empty body at offset {offset} — stopping.\")\n",
    "        break\n",
    "\n",
    "    r.raise_for_status()\n",
    "\n",
    "    try:\n",
    "        payload = r.json()\n",
    "    except Exception:\n",
    "        print(f\"⛔ Non-JSON response at offset {offset} — stopping.\")\n",
    "        break\n",
    "\n",
    "    page = payload.get(\"data\", [])\n",
    "    print(f\"page: {page}\")\n",
    "    if not page:\n",
    "        print(f\"✓ Finished at offset {offset}\")\n",
    "        break\n",
    "\n",
    "    all_current.extend(page)\n",
    "    offset += limit\n",
    "    time.sleep(0.2)   # throttle: voorkomt random 503s\n",
    "\n",
    "print(f\"Fetched {len(all_current)} current MEPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e9acd-9964-41de-84d7-79d161c6e6a1",
   "metadata": {},
   "source": [
    "## Store current MEPs in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccd95806-36b3-45e6-84eb-3cc0d11e4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"identifier\", StringType()),\n",
    "    StructField(\"label\", StringType()),\n",
    "    StructField(\"familyName\", StringType()),\n",
    "    StructField(\"givenName\", StringType()),\n",
    "    StructField(\"sortLabel\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"political_group\", StringType()),\n",
    "    StructField(\"load_ts\", TimestampType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eec4ca5b-6fa1-4021-ad4b-c3d9f4d1038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def normalize(mep):\n",
    "    return (\n",
    "        mep.get(\"id\"),\n",
    "        mep.get(\"type\"),\n",
    "        mep.get(\"identifier\"),\n",
    "        mep.get(\"label\"),\n",
    "        mep.get(\"familyName\"),\n",
    "        mep.get(\"givenName\"),\n",
    "        mep.get(\"sortLabel\"),\n",
    "        mep.get(\"api:country-of-representation\"),\n",
    "        mep.get(\"api:political-group\"),\n",
    "        datetime.utcnow()\n",
    "    )\n",
    "\n",
    "mep_records = [normalize(mep) for mep in all_current]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d91e65c5-fe28-4781-b27a-e62c3ec0e4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"TRUNCATE TABLE nessie.europarl.current_meps\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bde8a4d-5c20-4929-b513-9bf54e73d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+----------+--------------+----------+---------+---------+-------+---------------+--------------------------+\n",
      "|id           |type  |identifier|label         |familyName|givenName|sortLabel|country|political_group|load_ts                   |\n",
      "+-------------+------+----------+--------------+----------+---------+---------+-------+---------------+--------------------------+\n",
      "|person/101039|Person|101039    |Paolo BORCHIA |Borchia   |Paolo    |BORCHIA  |IT     |PfE            |2026-01-13 11:16:15.509369|\n",
      "|person/101585|Person|101585    |Niels FUGLSANG|Fuglsang  |Niels    |FUGLSANG |DK     |S&D            |2026-01-13 11:16:15.509386|\n",
      "|person/103246|Person|103246    |Auke ZIJLSTRA |Zijlstra  |Auke     |ZIJLSTRA |NL     |PfE            |2026-01-13 11:16:15.509394|\n",
      "|person/103381|Person|103381    |Terry REINTKE |Reintke   |Terry    |REINTKE  |DE     |Verts/ALE      |2026-01-13 11:16:15.509403|\n",
      "|person/1294  |Person|1294      |Elio DI RUPO  |Di Rupo   |Elio     |DIRUPO   |BE     |S&D            |2026-01-13 11:16:15.509406|\n",
      "+-------------+------+----------+--------------+----------+---------+---------+-------+---------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_curmeps = spark.createDataFrame(mep_records, schema)\n",
    "df_curmeps.count(), df_curmeps.show(5, False)\n",
    "\n",
    "df_curmeps.writeTo(\"nessie.europarl.current_meps\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4145c3a-dbce-4b77-b08f-3fcb81e35915",
   "metadata": {},
   "source": [
    "## Let's do some analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b379ea-9738-4596-9a37-e91605565a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|     DE|   96|\n",
      "|     FR|   81|\n",
      "|     IT|   76|\n",
      "|     ES|   60|\n",
      "|     PL|   53|\n",
      "|     RO|   33|\n",
      "|     NL|   31|\n",
      "|     BE|   22|\n",
      "|     CZ|   21|\n",
      "|     PT|   21|\n",
      "|     GR|   21|\n",
      "|     SE|   21|\n",
      "|     HU|   21|\n",
      "|     AT|   20|\n",
      "|     BG|   17|\n",
      "|     FI|   15|\n",
      "|     SK|   15|\n",
      "|     DK|   15|\n",
      "|     IE|   14|\n",
      "|     HR|   12|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"nessie.europarl.current_meps\")\n",
    "\n",
    "country_counts = (\n",
    "    df.groupBy(\"country\")\n",
    "      .count()\n",
    "      .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "country_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77138e9-ac6d-43d2-8924-47b1c3395722",
   "metadata": {},
   "source": [
    "# Meetings and Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd31f03-c5e4-48c4-85b9-6d95ea0ce005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create current_meps table if it doesn't exist\n",
    "spark.sql(\"\"\"CREATE TABLE nessie.europarl.meetings (\n",
    "   id STRING,\n",
    "   type STRING,\n",
    "   activity_date STRING,\n",
    "   activity_end_date STRING,\n",
    "   activity_id STRING,\n",
    "   activity_label_en STRING,\n",
    "   activity_start_date STRING,\n",
    "   had_activity_type STRING,\n",
    "   parliamentary_term STRING,\n",
    "   hasLocality STRING,\n",
    "   load_ts TIMESTAMP\n",
    ")\"\"\")\n",
    "print(\"✓ Created table: nessie.europarl.meetings\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE nessie.europarl.meeting_persons (\n",
    "   id STRING,\n",
    "   activity_id STRING,\n",
    "   personid STRING,\n",
    "   was_participant STRING,\n",
    "   was_excused STRING,\n",
    "   load_ts TIMESTAMP\n",
    ")\"\"\")\n",
    "print(\"✓ Created table: nessie.europarl.meeting_persons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ecd146c-7508-433b-a6a9-73074f4cf44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset: 0\n",
      "type(page): <class 'list'>\n",
      "offset: 100\n",
      "Fetched 53 meetings\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "from datetime import datetime\n",
    "\n",
    "BASE = \"https://data.europarl.europa.eu/api/v2/meetings\"\n",
    "\n",
    "headers = {\"Accept\": \"application/ld+json\"}\n",
    "\n",
    "def fetch_meetings(year=2025):\n",
    "    offset, limit = 0, 100\n",
    "    all_rows = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"offset: {offset}\")\n",
    "        params = {\"year\": year, \"offset\": offset, \"limit\": limit}\n",
    "        r = requests.get(BASE, headers=headers, params=params)\n",
    "\n",
    "        if not r.text.strip():\n",
    "            break\n",
    "\n",
    "        payload = r.json()\n",
    "        page = payload.get(\"data\", [])\n",
    "        print(f\"type(page): {type(page)}\")\n",
    "        # print(f\"page[0]: {page[0]}\")\n",
    "        if not page:\n",
    "            break\n",
    "\n",
    "        all_rows.extend(page)\n",
    "        offset += limit\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "meetings_raw = fetch_meetings(2025)\n",
    "print(f\"Fetched {len(meetings_raw)} meetings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959f0d1-10ec-4565-949a-113bcda12fc5",
   "metadata": {},
   "source": [
    "### Meeting schema and records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615a8f15-d0db-4fa1-b508-f99e05da0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(val):\n",
    "    if isinstance(val, dict):\n",
    "        return val.get(\"label\", {}).get(\"en\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9336f76-7580-46ae-b917-a86cb78f5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"TRUNCATE TABLE nessie.europarl.meetings\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2de3612e-f2b4-4ed6-89fc-e38bacc09b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "meeting_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"activity_date\", StringType()),\n",
    "    StructField(\"activity_end_date\", StringType()),\n",
    "    StructField(\"activity_id\", StringType()),\n",
    "    StructField(\"activity_label_en\", StringType()),\n",
    "    StructField(\"activity_start_date\", StringType()),\n",
    "    StructField(\"had_activity_type\", StringType()),\n",
    "    StructField(\"parliamentary_term\", StringType()),\n",
    "    StructField(\"hasLocality\", StringType()),\n",
    "    StructField(\"load_ts\", TimestampType())\n",
    "])\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def normalize_meeting(m):\n",
    "    return (\n",
    "        m.get(\"id\"),\n",
    "        m.get(\"type\"),\n",
    "        m.get(\"activity_date\"),\n",
    "        m.get(\"activity_end_date\"),\n",
    "        m.get(\"activity_id\"),\n",
    "        m.get(\"activity_label\", {}).get(\"en\"),\n",
    "        m.get(\"activity_start_date\"),\n",
    "        m.get(\"had_activity_type\"),\n",
    "        m.get(\"parliamentary_term\"),\n",
    "         m.get(\"hasLocality\") or m.get(\"has_locality\"),\n",
    "        datetime.utcnow()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b60e5b26-1c7b-4016-9046-21b5ddecd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_rows = [normalize_meeting(m) for m in meetings_raw]\n",
    "\n",
    "df_meetings = spark.createDataFrame(meeting_rows, meeting_schema)\n",
    "\n",
    "df_meetings.writeTo(\"nessie.europarl.meetings\").createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "170d9714-0869-47ef-80af-c014d198c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+-------------+-------------------------+-----------------+--------------------------+-------------------------+---------------------------------+------------------+--------------------------------------------------------------+--------------------------+\n",
      "|id                            |type    |activity_date|activity_end_date        |activity_id      |activity_label_en         |activity_start_date      |had_activity_type                |parliamentary_term|hasLocality                                                   |load_ts                   |\n",
      "+------------------------------+--------+-------------+-------------------------+-----------------+--------------------------+-------------------------+---------------------------------+------------------+--------------------------------------------------------------+--------------------------+\n",
      "|eli/dl/event/MTG-PL-2025-01-20|Activity|2025-01-20   |2025-01-20T23:00:00+01:00|MTG-PL-2025-01-20|Monday, 20 January 2025   |2025-01-20T01:00:00+01:00|def/ep-activities/PLENARY_SITTING|org/ep-10         |http://publications.europa.eu/resource/authority/place/FRA_SXB|2026-01-13 21:12:35.890862|\n",
      "|eli/dl/event/MTG-PL-2025-01-21|Activity|2025-01-21   |2025-01-21T23:00:00+01:00|MTG-PL-2025-01-21|Tuesday, 21 January 2025  |2025-01-21T01:00:00+01:00|def/ep-activities/PLENARY_SITTING|org/ep-10         |http://publications.europa.eu/resource/authority/place/FRA_SXB|2026-01-13 21:12:35.89087 |\n",
      "|eli/dl/event/MTG-PL-2025-01-22|Activity|2025-01-22   |2025-01-22T23:00:00+01:00|MTG-PL-2025-01-22|Wednesday, 22 January 2025|2025-01-22T01:00:00+01:00|def/ep-activities/PLENARY_SITTING|org/ep-10         |http://publications.europa.eu/resource/authority/place/FRA_SXB|2026-01-13 21:12:35.890888|\n",
      "|eli/dl/event/MTG-PL-2025-01-23|Activity|2025-01-23   |2025-01-23T23:00:00+01:00|MTG-PL-2025-01-23|Thursday, 23 January 2025 |2025-01-23T01:00:00+01:00|def/ep-activities/PLENARY_SITTING|org/ep-10         |http://publications.europa.eu/resource/authority/place/FRA_SXB|2026-01-13 21:12:35.890891|\n",
      "|eli/dl/event/MTG-PL-2025-01-29|Activity|2025-01-29   |2025-01-29T23:00:00+01:00|MTG-PL-2025-01-29|Wednesday, 29 January 2025|2025-01-29T01:00:00+01:00|def/ep-activities/PLENARY_SITTING|org/ep-10         |http://publications.europa.eu/resource/authority/place/BEL_BRU|2026-01-13 21:12:35.890895|\n",
      "+------------------------------+--------+-------------+-------------------------+-----------------+--------------------------+-------------------------+---------------------------------+------------------+--------------------------------------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meetings.count(), df_meetings.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc07339-16ce-47cd-9ddd-bb60ee58e4a0",
   "metadata": {},
   "source": [
    "### Meeting participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c29fef-b034-4cda-b747-0d10df5ba065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier(val):\n",
    "    if isinstance(val, dict):\n",
    "        return val.get(\"identifier\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454269d2-6aea-4ef3-8dc3-a5d6c0d4fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "person_rows = []\n",
    "\n",
    "for m in meetings_raw:\n",
    "    meeting_id = m.get(\"id\")\n",
    "    activity_id = m.get(\"activity_id\")\n",
    "\n",
    "    # participants\n",
    "    for p in m.get(\"had_participant_person\", []):\n",
    "        person_rows.append((\n",
    "            meeting_id,\n",
    "            activity_id,\n",
    "            p,            # p is nu gewoon 'person/197537'\n",
    "            \"true\",\n",
    "            \"false\",\n",
    "            datetime.utcnow()\n",
    "        ))\n",
    "\n",
    "    # excused\n",
    "    for p in m.get(\"had_excused_person\", []):\n",
    "        person_rows.append((\n",
    "            meeting_id,\n",
    "            activity_id,\n",
    "            p,\n",
    "            \"false\",\n",
    "            \"true\",\n",
    "            datetime.utcnow()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ace715-9079-4d3f-9b2b-c9ed7d047fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Meeting participants table written\n"
     ]
    }
   ],
   "source": [
    "person_schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"activity_id\", StringType()),\n",
    "    StructField(\"personid\", StringType()),\n",
    "    StructField(\"was_participant\", StringType()),\n",
    "    StructField(\"was_excused\", StringType()),\n",
    "    StructField(\"load_ts\", TimestampType())\n",
    "])\n",
    "\n",
    "df_persons = spark.createDataFrame(person_rows, person_schema)\n",
    "\n",
    "# Write to Iceberg\n",
    "df_persons.writeTo(\"nessie.europarl.meeting_persons\").createOrReplace()\n",
    "print(\"✓ Meeting participants table written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12a47d-2e2a-425f-9f62-bdd8630f38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons.count(), df_persons.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89bf8d-046c-4796-a5ce-981642e73004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
